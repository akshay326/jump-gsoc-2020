{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d53b7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "using JuMP\n",
    "import DiffOpt\n",
    "import HiGHS\n",
    "import ChainRulesCore\n",
    "import Flux\n",
    "import MLDatasets\n",
    "import Statistics\n",
    "import Base.Iterators: repeated\n",
    "using LinearAlgebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "afd3ecd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix_relu (generic function with 1 method)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function matrix_relu(\n",
    "    y::Matrix;\n",
    "    model = Model(() -> DiffOpt.diff_optimizer(HiGHS.Optimizer))\n",
    ")\n",
    "    N, M = size(y)\n",
    "    empty!(model)\n",
    "    set_silent(model)\n",
    "    @variable(model, x[1:N, 1:M] >= 0)\n",
    "    @objective(model, Min, x[:]'x[:] -2y[:]'x[:])\n",
    "    optimize!(model)\n",
    "    return value.(x)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c10ff154",
   "metadata": {},
   "outputs": [],
   "source": [
    "function ChainRulesCore.rrule(::typeof(matrix_relu), y::Matrix{T}) where T\n",
    "    model = Model(() -> DiffOpt.diff_optimizer(HiGHS.Optimizer))\n",
    "    pv = matrix_relu(y, model = model)\n",
    "    function pullback_matrix_relu(dl_dx)\n",
    "        # some value from the backpropagation (e.g., loss) is denoted by `l`\n",
    "        # so `dl_dy` is the derivative of `l` wrt `y`\n",
    "        x = model[:x] ## load decision variable `x` into scope\n",
    "        dl_dy = zeros(T, size(dl_dx))\n",
    "        dl_dq = zeros(T, size(dl_dx))\n",
    "        # set sensitivities\n",
    "        MOI.set.(model, DiffOpt.BackwardInVariablePrimal(), x[:], dl_dx[:])\n",
    "        # compute grad\n",
    "        DiffOpt.backward(model)\n",
    "        # return gradient wrt objective function parameters\n",
    "        obj_exp = MOI.get(model, DiffOpt.BackwardOutObjective())\n",
    "        # coeff of `x` in q'x = -2y'x\n",
    "        dl_dq[:] .= JuMP.coefficient.(obj_exp, x[:])\n",
    "        dq_dy = -2 ## dq/dy = -2\n",
    "        dl_dy[:] .= dl_dq[:] * dq_dy\n",
    "        return (ChainRulesCore.NoTangent(), dl_dy,)\n",
    "    end\n",
    "    return pv, pullback_matrix_relu\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "844c9441",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 30\n",
    "imgs = MLDatasets.MNIST.traintensor(1:N)\n",
    "labels = MLDatasets.MNIST.trainlabels(1:N);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09df3ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = float.(reshape(imgs, size(imgs, 1) * size(imgs, 2), N)) ## stack all the images\n",
    "train_Y = Flux.onehotbatch(labels, 0:9);\n",
    "\n",
    "test_imgs = MLDatasets.MNIST.testtensor(1:N)\n",
    "test_X = float.(reshape(test_imgs, size(test_imgs, 1) * size(test_imgs, 2), N))\n",
    "test_Y = Flux.onehotbatch(MLDatasets.MNIST.testlabels(1:N), 0:9);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "57950456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  Dense(784, 50, relu),                 \u001b[90m# 39_250 parameters\u001b[39m\n",
       "  Dense(50, 10),                        \u001b[90m# 510 parameters\u001b[39m\n",
       "  NNlib.softmax,\n",
       ")\u001b[90m                   # Total: 4 arrays, \u001b[39m39_760 parameters, 155.562 KiB."
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = Flux.Chain(\n",
    "    Flux.Dense(784, 50, Flux.relu), #784 being image linear dimension (28 x 28)\n",
    "    Flux.Dense(50, 10), # 10 beinf the number of outcomes (0 to 9)\n",
    "    Flux.softmax,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "4b1a5c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: (10, 1000)\n",
      "└ @ Main In[197]:1\n",
      "┌ Info: (10, 1000)\n",
      "└ @ Main In[197]:2\n"
     ]
    }
   ],
   "source": [
    "@info size(m(train_X))\n",
    "@info size(train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "be21036a",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "# epochs = 50 # ~1 minute (i7 8th gen with 16gb RAM)\n",
    "# epochs = 100 # leads to 77.8% in about 2 minutes\n",
    "\n",
    "dataset = repeated((train_X, train_Y), epochs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a06a334a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#42 (generic function with 1 method)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_loss(x, y) = Flux.crossentropy(m(x), y)\n",
    "opt = Flux.ADAM()\n",
    "evalcb = () -> @show(custom_loss(train_X, train_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "97c1b127",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom_loss(train_X, train_Y) = 2.2152708f0\n",
      "  0.232270 seconds (362.10 k allocations: 43.637 MiB, 78.62% compilation time)\n"
     ]
    }
   ],
   "source": [
    "@time Flux.train!(custom_loss, Flux.params(m), dataset, opt, cb = Flux.throttle(evalcb, 5));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7ec9f50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(x, y) = Statistics.mean(Flux.onecold(m(x)) .== Flux.onecold(y));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5eeb4588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.422"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "163d32ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(test_X, test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d9c0f0",
   "metadata": {},
   "source": [
    "## using svm at last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fb8c0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "struct SVM{M<:AbstractMatrix, B}\n",
    "    weight::M\n",
    "    bias::B\n",
    "    alpha::Number\n",
    "    function SVM(W::M, b::B, α) where {M<:AbstractMatrix, B}\n",
    "        new{M,B}(W, b, α)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db1ce1a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVM"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function SVM(in::Integer, out::Integer; alpha=0.05)\n",
    "    W = Flux.glorot_uniform(out, in)\n",
    "    b = Flux.glorot_uniform(out)\n",
    "    \n",
    "    return SVM(W, b, alpha)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36a68f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "function (svm::SVM)(x::Matrix; model = Model(() -> DiffOpt.diff_optimizer(HiGHS.Optimizer)))\n",
    "    W, b, alpha = svm.weight, svm.bias, svm.alpha\n",
    "    \n",
    "    N, M = size(x)\n",
    "    empty!(model)\n",
    "    set_silent(model)\n",
    "    \n",
    "    @variable(model, y[1:10, 1:M]) # should be 10 x 1000\n",
    "    \n",
    "#     @variable(model, e[1:10, 1:M]) # slack variables  \n",
    "#     @constraint(model, cons[i in 1:M], e[:, i] .== (y[:, i] - w*x[:, i] - b))\n",
    "\n",
    "    # objective minimizing squared error and ridge penalty\n",
    "    @objective(\n",
    "        model,\n",
    "        Min,\n",
    "        dot((W*x.+b.-y), (W*x.+b.-y)) + alpha * dot(W, W),\n",
    "    )\n",
    "    \n",
    "    optimize!(model)\n",
    "    return value.(y)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "97554380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "svm (generic function with 1 method)"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function svm(\n",
    "#     x::Matrix;\n",
    "#     model = Model(() -> DiffOpt.diff_optimizer(HiGHS.Optimizer)),\n",
    "#     alpha = 0.01,\n",
    "# )\n",
    "#     N, M = size(x) # 50-1000\n",
    "#     empty!(model)\n",
    "#     set_silent(model)\n",
    "    \n",
    "#     @variable(model, w[1:10, 1:N]) # angular coefficient\n",
    "#     @variable(model, b[1:10]) # linear coefficient\n",
    "#     @variable(model, y[1:10, 1:M]) # should be 10 x 1000\n",
    "    \n",
    "# #     @variable(model, e[1:10, 1:M]) # slack variables  \n",
    "# #     @constraint(model, cons[i in 1:M], e[:, i] .== (y[:, i] - w*x[:, i] - b))\n",
    "\n",
    "#     # objective minimizing squared error and ridge penalty\n",
    "#     @objective(\n",
    "#         model,\n",
    "#         Min,\n",
    "#         dot((w*x.+b.-y), (w*x.+b.-y)) + alpha * dot(w, w),\n",
    "#     )\n",
    "    \n",
    "#     optimize!(model)\n",
    "#     return value.(y)\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "496d3c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "function ChainRulesCore.rrule(svm::typeof(SVM), x::Matrix{T}) where T\n",
    "    model = Model(() -> DiffOpt.diff_optimizer(HiGHS.Optimizer))\n",
    "    py = svm(x, model = model)\n",
    "    \n",
    "    function pullback_svm(dl_dy)\n",
    "        # some value from the backpropagation (e.g., loss) is denoted by `l`\n",
    "        # so `dl_dy` is the derivative of `l` wrt `y`\n",
    "        y = model[:y]\n",
    "        W, b = svm.weight, svm.bias\n",
    "        N, M = size(x)\n",
    "        \n",
    "        MOI.set(\n",
    "            model,\n",
    "            DiffOpt.BackwardInVariablePrimal(),\n",
    "            y,\n",
    "            dl_dy\n",
    "        )\n",
    "        DiffOpt.backward(model)\n",
    "        obj = MOI.get(\n",
    "            model,\n",
    "            DiffOpt.BackwardOutObjective(),\n",
    "        )\n",
    "        \n",
    "        @info obj\n",
    "        \n",
    "        # dl_dx is needed by the previous dense layer\n",
    "        return (ChainRulesCore.NoTangent(), dl_dx,)\n",
    "    end\n",
    "    \n",
    "    return py, pullback_svm\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b84ff294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  Dense(784, 50, relu),                 \u001b[90m# 39_250 parameters\u001b[39m\n",
       "  SVM{Matrix{Float32}, Vector{Float32}}(Float32[-0.20215495 0.16586693 … -0.08584728 0.15465824; 0.14348093 0.19293313 … 0.13694264 -0.05919899; … ; -0.09913504 0.30569953 … 0.24543075 0.030610574; 0.012736179 -0.2579372 … 0.02508468 0.24031003], Float32[-0.6149052, 0.040175453, 0.7128812, -0.30544832, 0.6230561, 0.14602259, 0.5525571, -0.5266073, -0.25275296, -0.053562045], 0.05),\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = Flux.Chain(\n",
    "    Flux.Dense(784, 50, Flux.relu), #784 being image linear dimension (28 x 28)\n",
    "#     Flux.Dense(50, 10), # 10 beinf the number of outcomes (0 to 9)\n",
    "    SVM(50, 10),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f6124c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: (10, 30)\n",
      "└ @ Main In[13]:1\n",
      "┌ Info: (10, 30)\n",
      "└ @ Main In[13]:2\n"
     ]
    }
   ],
   "source": [
    "@info size(m(train_X)) # should be same sizes\n",
    "@info size(train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19c4d5b1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "Compiling Tuple{typeof(MathOptInterface.add_variable), MathOptInterface.Utilities.CachingOptimizer{MathOptInterface.Bridges.LazyBridgeOptimizer{DiffOpt.Optimizer{MathOptInterface.Utilities.CachingOptimizer{MathOptInterface.Bridges.LazyBridgeOptimizer{HiGHS.Optimizer}, MathOptInterface.Utilities.UniversalFallback{MathOptInterface.Utilities.Model{Float64}}}}}, MathOptInterface.Utilities.UniversalFallback{MathOptInterface.Utilities.Model{Float64}}}}: try/catch is not supported.",
     "output_type": "error",
     "traceback": [
      "Compiling Tuple{typeof(MathOptInterface.add_variable), MathOptInterface.Utilities.CachingOptimizer{MathOptInterface.Bridges.LazyBridgeOptimizer{DiffOpt.Optimizer{MathOptInterface.Utilities.CachingOptimizer{MathOptInterface.Bridges.LazyBridgeOptimizer{HiGHS.Optimizer}, MathOptInterface.Utilities.UniversalFallback{MathOptInterface.Utilities.Model{Float64}}}}}, MathOptInterface.Utilities.UniversalFallback{MathOptInterface.Utilities.Model{Float64}}}}: try/catch is not supported.",
      "",
      "Stacktrace:",
      "  [1] error(s::String)",
      "    @ Base ./error.jl:33",
      "  [2] instrument(ir::IRTools.Inner.IR)",
      "    @ Zygote ~/.julia/packages/Zygote/H6vD3/src/compiler/reverse.jl:121",
      "  [3] #Primal#19",
      "    @ ~/.julia/packages/Zygote/H6vD3/src/compiler/reverse.jl:202 [inlined]",
      "  [4] Zygote.Adjoint(ir::IRTools.Inner.IR; varargs::Nothing, normalise::Bool)",
      "    @ Zygote ~/.julia/packages/Zygote/H6vD3/src/compiler/reverse.jl:315",
      "  [5] _generate_pullback_via_decomposition(T::Type)",
      "    @ Zygote ~/.julia/packages/Zygote/H6vD3/src/compiler/emit.jl:101",
      "  [6] #s3051#1213",
      "    @ ~/.julia/packages/Zygote/H6vD3/src/compiler/interface2.jl:28 [inlined]",
      "  [7] var\"#s3051#1213\"(::Any, ctx::Any, f::Any, args::Any)",
      "    @ Zygote ./none:0",
      "  [8] (::Core.GeneratedFunctionStub)(::Any, ::Vararg{Any, N} where N)",
      "    @ Core ./boot.jl:571",
      "  [9] _pullback",
      "    @ ~/.julia/packages/JuMP/0STkJ/src/variables.jl:1042 [inlined]",
      " [10] _pullback(::Zygote.Context, ::typeof(JuMP._moi_add_variable), ::MathOptInterface.Utilities.CachingOptimizer{MathOptInterface.Bridges.LazyBridgeOptimizer{DiffOpt.Optimizer{MathOptInterface.Utilities.CachingOptimizer{MathOptInterface.Bridges.LazyBridgeOptimizer{HiGHS.Optimizer}, MathOptInterface.Utilities.UniversalFallback{MathOptInterface.Utilities.Model{Float64}}}}}, MathOptInterface.Utilities.UniversalFallback{MathOptInterface.Utilities.Model{Float64}}}, ::Model, ::ScalarVariable{Float64, Float64, Float64, Float64}, ::String)",
      "    @ Zygote ~/.julia/packages/Zygote/H6vD3/src/compiler/interface2.jl:0",
      " [11] _pullback",
      "    @ ~/.julia/packages/JuMP/0STkJ/src/variables.jl:1038 [inlined]",
      " [12] _pullback(::Zygote.Context, ::typeof(add_variable), ::Model, ::ScalarVariable{Float64, Float64, Float64, Float64}, ::String)",
      "    @ Zygote ~/.julia/packages/Zygote/H6vD3/src/compiler/interface2.jl:0",
      " [13] _pullback",
      "    @ ~/.julia/packages/JuMP/0STkJ/src/Containers/macro.jl:304 [inlined]",
      " [14] _pullback(::Zygote.Context, ::var\"#3#6\"{Model}, ::Int64, ::Int64)",
      "    @ Zygote ~/.julia/packages/Zygote/H6vD3/src/compiler/interface2.jl:0",
      " [15] _apply(::Function, ::Vararg{Any, N} where N)",
      "    @ Core ./boot.jl:804",
      " [16] adjoint",
      "    @ ~/.julia/packages/Zygote/H6vD3/src/lib/lib.jl:200 [inlined]",
      " [17] _pullback",
      "    @ ~/.julia/packages/ZygoteRules/AIbCs/src/adjoint.jl:65 [inlined]",
      " [18] _pullback",
      "    @ ~/.julia/packages/JuMP/0STkJ/src/Containers/container.jl:72 [inlined]",
      " [19] _pullback(ctx::Zygote.Context, f::JuMP.Containers.var\"#37#38\"{var\"#3#6\"{Model}}, args::Tuple{Int64, Int64})",
      "    @ Zygote ~/.julia/packages/Zygote/H6vD3/src/compiler/interface2.jl:0",
      " [20] #554",
      "    @ ~/.julia/packages/Zygote/H6vD3/src/lib/array.jl:197 [inlined]",
      " [21] iterate",
      "    @ ./generator.jl:47 [inlined]",
      " [22] collect(itr::Base.Generator{JuMP.Containers.VectorizedProductIterator{Tuple{Base.OneTo{Int64}, Base.OneTo{Int64}}}, Zygote.var\"#554#558\"{Zygote.Context, JuMP.Containers.var\"#37#38\"{var\"#3#6\"{Model}}}})",
      "    @ Base ./array.jl:681",
      " [23] map",
      "    @ ./abstractarray.jl:2323 [inlined]",
      " [24] ∇map(cx::Zygote.Context, f::JuMP.Containers.var\"#37#38\"{var\"#3#6\"{Model}}, args::JuMP.Containers.VectorizedProductIterator{Tuple{Base.OneTo{Int64}, Base.OneTo{Int64}}})",
      "    @ Zygote ~/.julia/packages/Zygote/H6vD3/src/lib/array.jl:197",
      " [25] _pullback",
      "    @ ~/.julia/packages/Zygote/H6vD3/src/lib/array.jl:244 [inlined]",
      " [26] _pullback",
      "    @ ./abstractarray.jl:2323 [inlined]",
      " [27] _pullback(::Zygote.Context, ::typeof(map), ::JuMP.Containers.var\"#37#38\"{var\"#3#6\"{Model}}, ::JuMP.Containers.VectorizedProductIterator{Tuple{Base.OneTo{Int64}, Base.OneTo{Int64}}})",
      "    @ Zygote ~/.julia/packages/Zygote/H6vD3/src/compiler/interface2.jl:0",
      " [28] _pullback",
      "    @ ~/.julia/packages/JuMP/0STkJ/src/Containers/container.jl:72 [inlined]",
      " [29] _pullback(::Zygote.Context, ::typeof(JuMP.Containers.container), ::var\"#3#6\"{Model}, ::JuMP.Containers.VectorizedProductIterator{Tuple{Base.OneTo{Int64}, Base.OneTo{Int64}}}, ::Type{Array})",
      "    @ Zygote ~/.julia/packages/Zygote/H6vD3/src/compiler/interface2.jl:0",
      " [30] _pullback",
      "    @ ~/.julia/packages/JuMP/0STkJ/src/Containers/container.jl:66 [inlined]",
      " [31] _pullback(::Zygote.Context, ::typeof(JuMP.Containers.container), ::var\"#3#6\"{Model}, ::JuMP.Containers.VectorizedProductIterator{Tuple{Base.OneTo{Int64}, Base.OneTo{Int64}}})",
      "    @ Zygote ~/.julia/packages/Zygote/H6vD3/src/compiler/interface2.jl:0",
      " [32] macro expansion",
      "    @ ~/.julia/packages/JuMP/0STkJ/src/macros.jl:142 [inlined]",
      " [33] _pullback",
      "    @ ./In[6]:8 [inlined]",
      " [34] _pullback(::Zygote.Context, ::var\"##_#2\", ::Model, ::SVM{Matrix{Float32}, Vector{Float32}}, ::Matrix{Float32})",
      "    @ Zygote ~/.julia/packages/Zygote/H6vD3/src/compiler/interface2.jl:0",
      " [35] _pullback",
      "    @ ./In[6]:2 [inlined]",
      " [36] _pullback(ctx::Zygote.Context, f::SVM{Matrix{Float32}, Vector{Float32}}, args::Matrix{Float32})",
      "    @ Zygote ~/.julia/packages/Zygote/H6vD3/src/compiler/interface2.jl:0",
      " [37] _pullback",
      "    @ ~/.julia/packages/Flux/7nTyc/src/layers/basic.jl:47 [inlined]",
      "--- the last 2 lines are repeated 1 more time ---",
      " [40] _pullback(::Zygote.Context, ::typeof(Flux.applychain), ::Tuple{Flux.Dense{typeof(NNlib.relu), Matrix{Float32}, Vector{Float32}}, SVM{Matrix{Float32}, Vector{Float32}}}, ::Matrix{Float32})",
      "    @ Zygote ~/.julia/packages/Zygote/H6vD3/src/compiler/interface2.jl:0",
      " [41] _pullback",
      "    @ ~/.julia/packages/Flux/7nTyc/src/layers/basic.jl:49 [inlined]",
      " [42] _pullback(ctx::Zygote.Context, f::Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu), Matrix{Float32}, Vector{Float32}}, SVM{Matrix{Float32}, Vector{Float32}}}}, args::Matrix{Float32})",
      "    @ Zygote ~/.julia/packages/Zygote/H6vD3/src/compiler/interface2.jl:0",
      " [43] _pullback",
      "    @ ./In[14]:4 [inlined]",
      " [44] _pullback(::Zygote.Context, ::typeof(custom_loss), ::Matrix{Float32}, ::Flux.OneHotArray{UInt32, 10, 1, 2, Vector{UInt32}})",
      "    @ Zygote ~/.julia/packages/Zygote/H6vD3/src/compiler/interface2.jl:0",
      " [45] _apply",
      "    @ ./boot.jl:804 [inlined]",
      " [46] adjoint",
      "    @ ~/.julia/packages/Zygote/H6vD3/src/lib/lib.jl:200 [inlined]",
      " [47] _pullback",
      "    @ ~/.julia/packages/ZygoteRules/AIbCs/src/adjoint.jl:65 [inlined]",
      " [48] _pullback",
      "    @ ~/.julia/packages/Flux/7nTyc/src/optimise/train.jl:110 [inlined]",
      " [49] _pullback(::Zygote.Context, ::Flux.Optimise.var\"#39#45\"{typeof(custom_loss), Tuple{Matrix{Float32}, Flux.OneHotArray{UInt32, 10, 1, 2, Vector{UInt32}}}})",
      "    @ Zygote ~/.julia/packages/Zygote/H6vD3/src/compiler/interface2.jl:0",
      " [50] pullback(f::Function, ps::Zygote.Params)",
      "    @ Zygote ~/.julia/packages/Zygote/H6vD3/src/compiler/interface.jl:352",
      " [51] gradient(f::Function, args::Zygote.Params)",
      "    @ Zygote ~/.julia/packages/Zygote/H6vD3/src/compiler/interface.jl:75",
      " [52] macro expansion",
      "    @ ~/.julia/packages/Flux/7nTyc/src/optimise/train.jl:109 [inlined]",
      " [53] macro expansion",
      "    @ ~/.julia/packages/Juno/n6wyj/src/progress.jl:134 [inlined]",
      " [54] train!(loss::Function, ps::Zygote.Params, data::Base.Iterators.Take{Base.Iterators.Repeated{Tuple{Matrix{Float32}, Flux.OneHotArray{UInt32, 10, 1, 2, Vector{UInt32}}}}}, opt::Flux.Optimise.ADAM; cb::Flux.var\"#throttled#70\"{Flux.var\"#throttled#66#71\"{Bool, Bool, var\"#17#18\", Int64}})",
      "    @ Flux.Optimise ~/.julia/packages/Flux/7nTyc/src/optimise/train.jl:107",
      " [55] top-level scope",
      "    @ ./timing.jl:210 [inlined]",
      " [56] top-level scope",
      "    @ ./In[14]:0",
      " [57] eval",
      "    @ ./boot.jl:360 [inlined]"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "dataset = repeated((train_X, train_Y), epochs)\n",
    "\n",
    "custom_loss(x, y) = Flux.crossentropy(m(x), y)\n",
    "opt = Flux.ADAM()\n",
    "evalcb = () -> @show(custom_loss(train_X, train_Y))\n",
    "\n",
    "@time Flux.train!(custom_loss, Flux.params(m), dataset, opt, cb = Flux.throttle(evalcb, 5));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "adb9e68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dd3576",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.5",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
